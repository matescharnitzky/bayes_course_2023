{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/bayes_course_2023/blob/master/notebooks/Section1_4-Model-Building-with-PyMC.ipynb)\n",
    "\n",
    "# Building Models in PyMC\n",
    "\n",
    "Now that we have been introduced to PyMC and Pytensor at a very high level, let's now take a more detailed look at PyMC's API as it relates to building models.\n",
    "\n",
    "Bayesian inference begins with specification of a probability model relating unknown variables to data. PyMC provides the basic building blocks for Bayesian probability models: stochastic random variables, deterministic variables, and factor potentials. \n",
    "\n",
    "A **stochastic random variable** is a factor whose value is not completely determined by its parents, while the value of a **deterministic random variable** is entirely determined by its parents. Most models can be constructed using only these two variable types. The third quantity, the **factor potential**, is *not* a variable but simply a\n",
    "log-likelihood term or constraint that is added to the joint log-probability to modify it. \n",
    "\n",
    "## The Distribution class\n",
    "\n",
    "A stochastic variable is represented in PyMC by a `Distribution` class. This structure adds functionality to Pytensor's `pytensor.tensor.random.op.RandomVariable` class, mainly by registering it with an associated PyMC `Model`. As we demonstrated in a previous section, `Distribution` objects are only usable inside of a `Model` context.\n",
    "\n",
    "`Distribution` subclasses (i.e. implementations of specific statistical distributions) will accept several arguments when constructed:\n",
    "\n",
    "`name`\n",
    ":   Name for the new model variable. This argument is **required**, and is used as a label and index value for the variable.\n",
    "\n",
    "`shape`\n",
    ":   The variable's shape.\n",
    "\n",
    "`total_size`\n",
    ":   The overall size of the variable (this variable will not exist for scalars).\n",
    "\n",
    "`dims`\n",
    ":   A tuple of dimension names known to the model.\n",
    "\n",
    "`transform`\n",
    ":   A transformation to be applied to the distribution when used by the model, especially when the distribution is constrained.\n",
    "\n",
    "`initval`\n",
    ":   Numeric or symbolic untransformed initial value of matching shape, or one of the following initial value strategies: \"moment\", \"prior\". Depending on the sampler's settings, a random jitter may be added to numeric, symbolic or moment-based initial values in the transformed space.\n",
    "\n",
    "`model`\n",
    ":   The PyMC model to which the variable belongs.\n",
    "\n",
    "\n",
    "As we previewed in the introduction, `Distribution` has a classmethod `dist` that returns a **stateless** probability distribution of that type; that is, without being wrapped in a PyMC random variable object. Sometimes we wish to use a particular statistical distribution, without using it as a variable in a model; for example, to generate random numbers from the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "x = pm.Exponential.dist(1)\n",
    "samples = pm.draw(x, draws=1000)\n",
    "sns.histplot(samples);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example: Educational Outcomes for Hearing-impaired Children\n",
    "\n",
    "To demonstrate the PyMC API in-depth, we will use a dataset of educational outcomes for children with hearing impairment. Here, we are interested in determining factors that are associated with better or poorer learning outcomes. \n",
    "\n",
    "This anonomized dataset is taken from the Listening and Spoken Language Data Repository (LSL-DR), an international data repository that tracks the demographics and longitudinal outcomes for children who have hearing loss and are enrolled programs focused on supporting listening and spoken language development. Researchers are interesting in discovering factors related to improvements in educational outcomes at within these programs.\n",
    "\n",
    "There is a suite of available predictors, including: \n",
    "\n",
    "* gender (`male`)\n",
    "* number of siblings in the household (`siblings`)\n",
    "* index of family involvement (`family_inv`)\n",
    "* whether the primary household language is not English (`non_english`)\n",
    "* presence of a previous disability (`prev_disab`)\n",
    "* non-white race (`non_white`)\n",
    "* age at the time of testing (in months, `age_test`)\n",
    "* whether hearing loss is not severe (`non_severe_hl`)\n",
    "* whether the subject's mother obtained a high school diploma or better (`mother_hs`)\n",
    "* whether the hearing impairment was identified by 3 months of age (`early_ident`).\n",
    "\n",
    "The outcome variable is a standardized test score in one of several learning domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>male</th>\n",
       "      <th>siblings</th>\n",
       "      <th>family_inv</th>\n",
       "      <th>non_english</th>\n",
       "      <th>prev_disab</th>\n",
       "      <th>age_test</th>\n",
       "      <th>non_severe_hl</th>\n",
       "      <th>mother_hs</th>\n",
       "      <th>early_ident</th>\n",
       "      <th>non_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  male  siblings  family_inv  non_english  prev_disab  age_test   \n",
       "0     40     0       2.0         2.0        False         NaN        55  \\\n",
       "1     31     1       0.0         NaN        False         0.0        53   \n",
       "2     83     1       1.0         1.0         True         0.0        52   \n",
       "3     75     0       3.0         NaN        False         0.0        55   \n",
       "5     62     0       0.0         4.0        False         1.0        50   \n",
       "\n",
       "   non_severe_hl  mother_hs  early_ident  non_white  \n",
       "0            1.0        NaN        False      False  \n",
       "1            0.0        0.0        False      False  \n",
       "2            1.0        NaN        False       True  \n",
       "3            0.0        1.0        False      False  \n",
       "5            0.0        NaN        False      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_scores = pd.read_csv(pm.get_data(\"test_scores.csv\"), index_col=0)\n",
    "test_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_scores[\"score\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing values is a very bad idea in general, but we do so here for simplicity\n",
    "X = test_scores.dropna().astype(float)\n",
    "y = X.pop(\"score\")\n",
    "\n",
    "# Standardize the features\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "\n",
    "N, D = X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "This is a **multivariate regression** model, which takes the form:\n",
    "\n",
    "$$\\begin{aligned} \n",
    "Y  &\\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n",
    "\\mu &= \\beta_0 + X \\beta\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $X$ is a matrix of predictors and $\\beta$ a vector of coefficients.\n",
    "\n",
    "However, while there are several potential predictors in the LSL-DR dataset, it is difficult *a priori* to determine which ones are relevant for constructing an effective statistical model. There are a number of approaches for conducting variable selection, but a popular automated method is *regularization*, whereby ineffective covariates are shrunk towards zero via regularization (a form of penalization) if they do not contribute to predicting outcomes. \n",
    "\n",
    "You may have heard of regularization from machine learning or classical statistics applications, where methods like the lasso or ridge regression shrink parameters towards zero by applying a penalty to the size of the regression parameters. In a Bayesian context, we apply an appropriate prior distribution to the regression coefficients. One such prior is the *hierarchical regularized horseshoe*, which uses two regularization strategies, one global and a set of local local parameters, one for each coefficient. The key to making this work is by selecting a long-tailed distribution as the shrinkage priors, which allows some to be nonzero, while pushing the rest towards zero.\n",
    "\n",
    "The horeshoe prior for each regression coefficient $\\beta_i$ looks like this:\n",
    "\n",
    "$$\\beta_i \\sim N\\left(0, \\tau^2 \\cdot \\tilde{\\lambda}_i^2\\right)$$\n",
    "\n",
    "where $\\sigma$ is the prior on the error standard deviation that will also be used for the model likelihood.\n",
    "\n",
    "Where $\\tau$ is the global shrinkage parameter and $\\tilde{\\lambda}_i$ is the local. Let's start global: for the prior on $\\tau$ we will use a Half-StudentT distribution, which is a reasonable choice becuase it is heavy-tailed.\n",
    "\n",
    "$$\n",
    "\\tau \\sim \\textrm{Half-StudentT}_{2} \\left(\\frac{D_0}{D - D_0} \\cdot \\frac{\\sigma}{\\sqrt{N}}\\right).\n",
    "$$\n",
    "\n",
    "One catch is that the parameterization of the prior requires a pre-specified value $D_0$, which represents the true number of non-zero coefficients. Fortunately, a reasonable guess at this value is all that is required, and it need only be within an order of magnitude of the true number. Let's use half the number of predictors as our guess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D0 = int(D / 2)\n",
    "D0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the local shrinkage parameters are defined by the ratio:\n",
    "\n",
    "$$\\tilde{\\lambda}_i^2 = \\frac{c^2 \\lambda_i^2}{c^2 + \\tau^2 \\lambda_i^2}.$$\n",
    "\n",
    "To complete this specification, we need priors on $\\lambda_i$ and $c$;  as with the global shrinkage, we use a long-tailed $\\textrm{Half-StudentT}_5(1)$  on the $\\lambda_i$. We need $c^2$ to be strictly positive, but not necessarily long-tailed, so an inverse gamma prior on $c^2$, $c^2 \\sim \\textrm{InverseGamma}(1, 1)$ fits the bill.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of stochastic random variables\n",
    "\n",
    "Stochastic random variables with standard distributions provided by PyMC can be created in a single line using special subclasses of the `Distribution` class. \n",
    "\n",
    "This model employs a couple of new distributions: the `HalfStudentT` distribution for the $\\tau$ and $\\lambda$ priors, and the `InverseGamma` distribution for the $c2$ variable.\n",
    "\n",
    "We are also going to take advantage of **named dimensions** in PyMC by passing the input variable names into the model as coordinates called \"predictors\". This will allow us to pass this vector of names as a replacement for the `shape` integer argument in the vector-valued parameters. The model will then associate the appropriate name with each latent parameter that it is estimating. This is a little more work to set up, but will pay dividends later when we are working with our model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "\n",
    "with pm.Model(coords={\"predictors\": X.columns.values}) as test_score_model:\n",
    "\n",
    "    # Prior on error SD\n",
    "    sigma = pm.HalfNormal(\"sigma\", 25)\n",
    "    \n",
    "    # Global shrinkage prior\n",
    "    tau = pm.HalfStudentT(\"tau\", 2, D0 / (D - D0) * sigma / np.sqrt(N))\n",
    "    # Local shrinkage prior\n",
    "    lam = pm.HalfStudentT(\"lam\", 2, dims=\"predictors\")\n",
    "    c2 = pm.InverseGamma(\"c2\", 1, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Contexts and Random Variables\n",
    "\n",
    "As we have seen, the canonical way to specify PyMC models is using a `Model` context manager. Generally speaking, a context manager is a Python idiom that does the following:\n",
    "\n",
    "```python\n",
    "    VAR = EXPR\n",
    "    VAR.__enter__()\n",
    "    try:\n",
    "        USERCODE\n",
    "    finally:\n",
    "        VAR.__exit__()\n",
    "\n",
    "```\n",
    "\n",
    "As an analogy, `Model` is a tape machine that records what is being added to the model; it keeps track the random variables (observed or unobserved) and other model components. The model context then computes some simple model properties, builds a **bijection** mapping that transforms between Python dictionaries and numpy/Pytensor ndarrays. , More importantly, a `Model` contains methods to compile Pytensor functions that take Random Variables--that are also\n",
    "initialised within the same model--as input.\n",
    "\n",
    "Within a model context, random variables are essentially Pytensor `TensorVariables`:\n",
    "\n",
    "```python\n",
    "with pm.Model() as model:\n",
    "    z = pm.Normal('z', mu=0., sigma=5.)             # ==> pytensor.tensor.var.TensorVariable\n",
    "    x = pm.Normal('x', mu=z, sigma=1., observed=5.) # ==> pytensor.tensor.var.TensorVariable\n",
    "pm.logp(z, 2.5).eval()                              # ==> -2.65337645\n",
    "model.compile_logp()({'z': 2.5})                    # ==> -6.6973152\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC includes most of the probability density functions (for continuous variables) and probability mass functions (for discrete variables) used in statistical modeling. These distributions are divided into five distinct categories:\n",
    "\n",
    "* Univariate continuous\n",
    "* Univariate discrete\n",
    "* Multivariate\n",
    "* Mixture\n",
    "* Timeseries\n",
    "\n",
    "Probability distributions are all subclasses of `Distribution`, which in turn has two major subclasses: `Discrete` and `Continuous`. In terms of data types, a `Continuous` random variable is given whichever floating point type is defined by `pytensor.config.floatX`, while `Discrete` variables are given `int16` types when `pytensor.config.floatX` is `float32`, and `int64` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate and Timeseries random variables are vector-valued, rather than scalar (though `Continuous` and `Discrete` variables may have non-scalar values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.shape.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam.shape.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the `Distribution` subclasses included in PyMC will have two key methods, `random()` and `logp()`, which are used to generate random values and compute the log-probability of a value, respectively.\n",
    "\n",
    "```python\n",
    "class SomeDistribution(Continuous):\n",
    "    def __init__(...):\n",
    "        ...\n",
    "\n",
    "    def random(self, point=None, size=None):\n",
    "        ...\n",
    "        return random_samples\n",
    "\n",
    "    def logp(self, value):\n",
    "        ...\n",
    "        return total_log_prob\n",
    "```\n",
    "\n",
    "PyMC expects the `logp()` method to return a log-probability evaluated at the passed `value` argument. This method is used internally by all of the inference methods to calculate the model log-probability that is used for fitting models. The `random()` method is used to simulate values from the variable, and is used internally for posterior predictive checks.\n",
    "\n",
    "Distributions will optionally have `cdf` and `icdf` methods, representing the cumulative distribution function and inverse cumulative distribution functions, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a well-behaved density function, we can use it in a model to build a model log-likelihood function. Almost any Pytensor function can be turned into a\n",
    "distribution using the `DensityDist` function. For exmaple, a uniformly-distributed discrete stochastic variable could be created manually from a function that computes its log-probability as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    \n",
    "    def uniform_logp(value, lower, upper):\n",
    "        return pm.math.switch((value > upper) | (value < lower), -np.inf, -pm.math.log(upper - lower + 1))\n",
    "\n",
    "    u = pm.DensityDist('u', 0, 10, logp=uniform_logp, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, 2).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing values outside the support of the distribution to `logp()` will return `-inf`, since the value has no probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.logp(u, -4).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of things to notice: while the function specified for the `logp` argument can be an arbitrary Python function, it must use **Pytensor operators and functions** in its body. This is because one or more of the arguments passed to the function may be `TensorVariables`, and they must be supported. \n",
    "\n",
    "To emphasize, the Python function passed to `DensityDist` should compute the *log*-density or *log*-probability of the variable. That is why the return value in the example above is `-log(upper-lower+1)` rather than `1/(upper-lower+1)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-transformation\n",
    "\n",
    "To support efficient sampling by PyMC's MCMC algorithms, any continuous variables that are constrained to a sub-interval of the real line are automatically transformed so that their support is unconstrained. This frees sampling algorithms from having to deal with boundary constraints.\n",
    "\n",
    "For example, if we look at the variables we have create in the test score model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_model.value_vars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's `value_vars` attribute stores the values of each random variable actually used by the model's log-likelihood.\n",
    "\n",
    "As the name suggests, the variables `sigma`, `tau`, `lam`, and `c2` have been log-transformed, and this is the space over which posterior sampling takes place. When a sample is drawn, the value of the transformed variable is simply back-transformed to recover the original variable.\n",
    "\n",
    "By default, auto-transformed variables are ignored when summarizing and plotting model output, since they are not generally of interest to the user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our model specification, the intercept will not be subject to regularization, and will be given a normal distribution centered on the population mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    beta0 = pm.Normal(\"beta0\", 100, 25.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Variables\n",
    "\n",
    "A deterministic variable is one whose values are **completely determined** by the values of their parents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "\n",
    "    shrinkage_sd = tau * lam * pm.math.sqrt(c2 / (c2 + tau**2 * lam**2))\n",
    "\n",
    "    beta = pm.Normal('beta', 0, shrinkage_sd, dims='predictors')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `shrinkage_sd`'s value can be computed exactly from the values of its parents `tau`, `lam` and `c2`.\n",
    "\n",
    "There are two types of deterministic variables in PyMC:\n",
    "\n",
    "#### Anonymous deterministic variables\n",
    "\n",
    "The easiest way to create a deterministic variable is to operate on or transform one or more variables in a model directly, as we have done above for `shrinkage_sd`.\n",
    "\n",
    "These are called *anonymous* variables because we did not wrap it with a call to `Determinstic`, which gives it a name as its first argument. We simply specified the variable as a Python (or, Pytensor) expression. This is therefore the simplest way to construct a determinstic variable. The only caveat is that the values generated by anonymous determinstics at every iteration of a MCMC algorithm, for example, are not recorded to the resulting trace. So, this approach is only appropriate for intermediate values in your model that you do not wish to obtain posterior estimates for, alongside the other variables in the model.\n",
    "\n",
    "#### Named deterministic variables\n",
    "\n",
    "To ensure that deterministic variables' values are accumulated during sampling, they should be instantiated using the **named deterministic** interface; this uses the `Deterministic` function to create the variable. Two things happen when a variable is created this way:\n",
    "\n",
    "1. The variable is given a name (passed as the first argument)\n",
    "2. The variable is appended to the model's list of random variables, which ensures that its values are tallied.\n",
    "\n",
    "If we wanted named variabels for the shrinkage standard deviation, we would have specified:\n",
    "\n",
    "```python\n",
    "shrinkage_sd = pm.Deterministic('shrinkage_sd', tau * lam * pm.math.sqrt(c2 / (c2 + tau**2 * lam**2)))\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Random Variables\n",
    "\n",
    "Stochastic random variables whose values are observed (*i.e.* data likelihoods) are represented by a different class than unobserved random variables. A `ObservedRV` object is instantiated any time a stochastic variable is specified with data passed as the `observed` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    \n",
    "    scores = pm.Normal(\"scores\", beta0 + X.values @ beta, sigma, observed=y.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important responsibility of `ObservedRV` is to automatically handle missing values in the data, when they are present (absent?). More on this later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Potentials\n",
    "\n",
    "For some applications, we want to be able to modify the joint density by incorporating terms that don't correspond to probabilities of variables conditional on parents. For example, suppose in the coal mining disasters model we want to constrain the difference between the early and late means to be less than 1, so that the joint density becomes: \n",
    "\n",
    "$$p(y,\\tau,\\lambda_1,\\lambda_2) \\propto p(y|\\tau,\\lambda_1,\\lambda_2) p(\\tau) p(\\lambda_1) p(\\lambda_2) I(|\\lambda_2-\\lambda_1| \\lt 1)$$\n",
    "\n",
    "We call such log-probability terms **factor potentials** (Jordan 2004). Bayesian\n",
    "hierarchical notation doesn't accomodate these potentials. \n",
    "\n",
    "### Creation of Potentials\n",
    "\n",
    "A potential can be created via the `Potential` function, in a way very similar to `Deterministic`'s named interface:\n",
    "\n",
    "```python\n",
    "with disaster_model:\n",
    "    \n",
    "    rate_constraint = pm.Potential('rate_constraint', pm.math.switch(pm.math.abs(early_mean-late_mean)>1, -np.inf, 0))\n",
    "```\n",
    "\n",
    "The function takes just a `name` as its first argument and an expression returning the appropriate log-probability as the second argument.\n",
    "\n",
    "A common use of a factor potential is to represent an observed likelihood, where the **observations are partly a function of model variables**. In the contrived example below, we are representing the error in a linear regression model as a zero-mean normal random variable. Thus, the \"data\" in this scenario is the residual, which is a function both of the data and the regression parameters. \n",
    "\n",
    "If we represent this as a standard likelihood function (a `Distribution` with an `observed` keyword argument), we run into problems. This parameterization would not be compatible with an observed stochastic, because the `err` term would become fixed in the likelihood and not be allowed to change during sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([15, 10, 16, 11, 9, 11, 10, 18, 11])\n",
    "x = np.array([1, 2, 4, 5, 6, 8, 19, 18, 12])\n",
    "\n",
    "with pm.Model() as regression:\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', 5)\n",
    "    beta = pm.Normal('beta', 0, sigma=2)\n",
    "    mu = pm.Normal('mu', 0, sigma=10)\n",
    "\n",
    "    err = y - (mu + beta*x)\n",
    "                  \n",
    "    like = pm.Normal('like', 0, sigma=sigma, observed=err)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can re-express the likelihood as a factor potential, which is a function of the data and the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([15, 10, 16, 11, 9, 11, 10, 18, 11])\n",
    "x = np.array([1, 2, 4, 5, 6, 8, 19, 18, 12])\n",
    "\n",
    "with pm.Model() as regression:\n",
    "\n",
    "    sigma = pm.HalfCauchy('sigma', 5)\n",
    "    beta = pm.Normal('beta', 0, sigma=2)\n",
    "    mu = pm.Normal('mu', 0, sigma=10)\n",
    "\n",
    "    err = y - (mu + beta*x)\n",
    "                  \n",
    "    like = pm.Potential('like', \n",
    "        pm.logp(\n",
    "            pm.Normal.dist(0, sigma=sigma), \n",
    "            err\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bioassay model\n",
    "\n",
    "Gelman et al. (2003) present an example of an acute toxicity test, commonly performed on animals to estimate the toxicity of various compounds.\n",
    "\n",
    "In this dataset `log_dose` includes 4 levels of dosage, on the log scale, each administered to 5 rats during the experiment. The response variable is death, the number of positive responses to the dosage.\n",
    "\n",
    "The number of deaths can be modeled as a binomial response, with the probability of death being a linear function of dose:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y_i &\\sim \\text{Bin}(n_i, p_i) \\\\\n",
    "\\text{logit}(p_i) &= a + b x_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "The common statistic of interest in such experiments is the LD50, the dosage at which the probability of death is 50%.\n",
    "\n",
    "Specify this model in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log dose in each group\n",
    "log_dose = [-.86, -.3, -.05, .73]\n",
    "\n",
    "# Sample size in each group\n",
    "n = 5\n",
    "\n",
    "# Outcomes\n",
    "deaths = [0, 1, 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with MCMC\n",
    "\n",
    "PyMC's core business is using Markov chain Monte Carlo to fit virtually any probability model. This involves the assignment and coordination of a suite of **step methods**, each of which is responsible for updating one or more variables. \n",
    "\n",
    "The user's interface to PyMC's sampling algorithms is the `sample` function:\n",
    "\n",
    "```python\n",
    "pm.sample(\n",
    "    draws: int = 1000,\n",
    "    *,\n",
    "    tune: int = 1000,\n",
    "    chains: Optional[int] = None,\n",
    "    cores: Optional[int] = None,\n",
    "    random_seed: Union[int, Sequence[int], numpy.ndarray, NoneType, numpy.random.mtrand.RandomState, numpy.random._generator.Generator] = None,\n",
    "    progressbar: bool = True,\n",
    "    step=None,\n",
    "    nuts_sampler: str = 'pymc',\n",
    "    initvals: Union[Dict[Union[pytensor.graph.basic.Variable, str], Union[numpy.ndarray, pytensor.graph.basic.Variable, str]], Sequence[Optional[Dict[Union[pytensor.graph.basic.Variable, str], Union[numpy.ndarray, pytensor.graph.basic.Variable, str]]]], NoneType] = None,\n",
    "    init: str = 'auto',\n",
    "    jitter_max_retries: int = 10,\n",
    "    n_init: int = 200000,\n",
    "    trace: Optional[pymc.backends.base.BaseTrace] = None,\n",
    "    discard_tuned_samples: bool = True,\n",
    "    compute_convergence_checks: bool = True,\n",
    "    keep_warning_stat: bool = False,\n",
    "    return_inferencedata: bool = True,\n",
    "    idata_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    nuts_sampler_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    callback=None,\n",
    "    mp_ctx=None,\n",
    "    model: Optional[pymc.model.Model] = None,\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "\n",
    "`sample` assigns particular samplers to model variables, and generates samples from them. The `draws` argument\n",
    "controls the total number of MCMC iterations. PyMC can automate most of the details of sampling, outside of the selection of the number of draws, using default settings for several parameters that control how the sampling is set up and conducted. However, users may manually intervene in the specification of the sampling by passing values to a number of keyword argumetns for `sample`.\n",
    "\n",
    "### Assigning step methods\n",
    "\n",
    "The `step` argument allows users to assign a MCMC sampling algorithm to the entire model, or to a subset of the variables in the model. For example, if we wanted to use the Metropolis-Hastings sampler to fit our model, we could pass an instance of that step method to `sample` via the `step` argument:\n",
    "\n",
    "```python\n",
    "with my_model:\n",
    "\n",
    "    trace = pm.sample(1000, step=pm.Metropolis())\n",
    "```\n",
    "\n",
    "or if we only wanted to assign `Metropolis` to a parameter called `beta`:\n",
    "\n",
    "```python\n",
    "with my_model:\n",
    "\n",
    "    trace = pm.sample(1000, step=pm.Metropolis(vars=[beta]))\n",
    "```\n",
    "\n",
    "Step method classes handle individual stochastic variables, or sometimes groups of them. They are responsible for making the variables they handle take **single MCMC steps** conditional on the rest of the model. Each PyMC step method (usually subclasses of `ArrayStep`) implements a method called `astep()`, which is called iteratively by `sample`. \n",
    "\n",
    "All step methods share an optional argument `vars` that allows a particular subset of variables to be handled by the step method instance. Particular step methods will have additional arguments for setting parameters and preferences specific to that sampling algorithm.\n",
    "\n",
    "> NB: when a PyMC function or method has an argument called `vars` it is expecting a list of variables (*i.e.* the variables themselves), whereas arguments called `var_names` expect a list of variables names (*i.e.* strings)\n",
    "\n",
    "#### HamiltonianMC\n",
    "\n",
    "The Hamiltonian Monte Carlo algorithm is implemented in the `HamiltonianMC` class. Being a gradient-based sampler, it is only suitable for **continuous random variables**. Several optional arguments can be provided by the user. The algorithm is **non-adaptive**, so the parameter values passed at instantiation are fixed at those values throughout sampling.\n",
    "\n",
    "`HamiltonianMC` requires a scaling matrix parameter `scaling`, which is analogous to the variance parameter for the jump proposal distribution in Metropolis-Hastings, although it is used somewhat differently here. The matrix gives an approximate shape of the posterior distribution, so that `HamiltonianMC` does not make jumps that are too large in some directions and too small in other directions. It is important to set this scaling parameter to a reasonable value to facilitate efficient sampling. This is especially true for models that have many unobserved stochastic random variables or models with highly non-normal posterior distributions. \n",
    "\n",
    "Fortunately, `HamiltonianMC` can often make good guesses for the scaling parameters. If you pass a point in parameter space (as a dictionary of variable names to parameter values, the same format as returned by `find_MAP`), it will look at the **local curvature** of the log posterior-density (the diagonal of the Hessian matrix) at that point to guess values for a good scaling vector, which can result in a good scaling value. Also, the MAP estimate is often a good point to use to initiate sampling. \n",
    "\n",
    "- `scaling` \n",
    ": Scaling for momentum distribution. If a 1-dimensional array is passed, it is interpreted as a matrix diagonal.\n",
    "            \n",
    "- `step_scale` \n",
    ": Size of steps to take, automatically scaled down by $1/n^{0.25}$. Defaults to .25.\n",
    "            \n",
    "- `path_length` \n",
    ": total length to travel during leapfrog. Defaults to 2.\n",
    "            \n",
    "- `is_cov` \n",
    ": Flag for treating scaling as a covariance matrix/vector, if True. Treated as precision otherwise.\n",
    "            \n",
    "- `step_rand` \n",
    ": A function which takes the step size and returns an new one used to randomize the step size at each iteration.\n",
    "\n",
    "\n",
    "#### NUTS\n",
    "\n",
    "`NUTS` is the No U-turn Sampler of Hoffman and Gelman (2014), an adaptive version of Hamiltonian MC that **automatically tunes** the step size and number on the fly. \n",
    "\n",
    "In addition to the arguments to `HamiltonianMC`, `NUTS` takes additional parameters to controls the tuning. The most important of these is the target acceptance rate for the Metropolis acceptance phase of the algorithm, `target_accept`. \n",
    "Sometimes if the NUTS struggles to sample efficiently, changing this parameter above the default target rate of 0.8 will improve sampling (the original recommendation by Hoffman & Gelman was 0.6). Increasing the rate very high will also make the sampler more conservative, however, taking many small steps at every iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    trace_90 = pm.sample(1000, tune=2000, target_accept=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_90.sample_stats['acceptance_rate'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is rarely a reason to use `HamiltonianMC` rather than `NUTS`. It is the default sampler for continuous variables in PyMC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis\n",
    "\n",
    "``Metropolis`` implements a Metropolis-Hastings step, as described the theory section, and is designed to handle float- and integer-valued variables.\n",
    "\n",
    "A `Metropolis` step method can be instantiated with any of several optional arguments:\n",
    "\n",
    "- `S`\n",
    ":   This sets the proposal standard deviation or covariance matrix.\n",
    "\n",
    "- `proposal_dist`\n",
    ":   A function that generates zero-mean random deviates used as proposals. Defaults to the normal distribution.\n",
    "\n",
    "- `scaling`\n",
    ":   An initial scale factor for the proposal\n",
    "\n",
    "- `tune_interval`\n",
    ":   The number of intervals between tuning updates to `scaling` factor.\n",
    "\n",
    "When the step method is instantiated, the `proposal_dist` is parameterized with the value passed for `S`. While sampling, the value of `scaling` is used to scale the value proposed by `proposal_dist`, and this value is tuned throughout the MCMC run. During tuning, the acceptance ratio of the step method is examined, and this scaling factor\n",
    "is updated accordingly. Tuning only occurs when the acceptance rate is **lower than 20%** or **higher than 50%**; rates between 20-50% are considered optimal for Metropolis-Hastings sampling. The default tuning interval (`tune_interval`) is 100 iterations.\n",
    "\n",
    "Although tuning will continue throughout the sampling loop, it is important to verify that the\n",
    "**diminishing tuning** condition of [Roberts and Rosenthal (2007)](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.jap/1183667414) is satisfied: the\n",
    "amount of tuning should decrease to zero, or tuning should become very infrequent.\n",
    "\n",
    "`Metropolis` handles discrete variable types automatically by rounding the proposed values and casting them to integers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BinaryMetropolis\n",
    "\n",
    "While binary (boolean) variables can be handled by the `Metropolis` step method, sampling will be very inefficient. The `BinaryMetropolis` class is optimized to handle binary variables, by one of only two possible values. The only tuneable parameter is the `scaling` argument, which is used to vary the Bernoulli probability:\n",
    "\n",
    "    p_jump = 1. - .5 ** self.scaling\n",
    "\n",
    "This value is compared to pseudo-random numbers generated by the step method, to determine whether a 0 or 1 is proposed.\n",
    "\n",
    "`BinaryMetropolis` will be automatically selected for random variables that are distributed as Bernoulli, or categorical with only 2 categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice\n",
    "\n",
    "Though the Metropolis-Hastings algorithm is easy to implement for a variety of models, its efficiency is poor. We have seen that it is possible to tune Metropolis samplers, but it would be nice to have a \"black-box\" method that works for arbitrary continuous distributions, which we may know little about a priori.\n",
    "\n",
    "The **slice sampler** (Neal 2003) improves upon the Metropolis sampler by being both efficient and easy to program generally. The idea is to first sample from the conditional distribution for $y$ (i.e., $Pr(x)$) given some current value of $x$, which is uniform over the $(0,f(x))$, and conditional on this value for $y$, then sample $x$, which is uniform on $S = {x : y < f (x)}$.\n",
    "\n",
    "In PyMC, the `Slice` class implements the **univariate** slice sampler. It is suitable for univariate, continuous variables. There is a single user-defined parameter `w`, which sets the width of the initial slice. If not specified, it defaults to a width of 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMC also includes implementations of stochastic gradient Markov chain Monte Carlo (SGMCMC, Nemeth & Fearnhead 2019) and Multi-Level Delayed Acceptance MCMC (MLDA, Dodwell et al. 2019), which we will not cover here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `step` is not specified by the user, PyMC will assign step methods to variables automatically. To do so, each step method implements a class method called `Competence`. This method returns a value from 0 (incompatible) to 3 (ideal), based on the attributes of the random variable in question. `sample()` assigns the step method that returns the highest competence value to each of its unallocated stochastic random variables. In general:\n",
    "\n",
    "* Binary variables will be assigned to `BinaryMetropolis` (Metropolis-Hastings for binary values)\n",
    "* Discrete variables will be assigned to `Metropolis`\n",
    "* Continuous variables will be assigned to `NUTS` (No U-turn Sampler)\n",
    "\n",
    "### Starting values\n",
    "\n",
    "The `initvals` argument allows for the specification of starting values for stochastic random variables in the model. MCMC algorithms begin by initializing all unknown quantities to arbitrary starting values. Though in theory the value can be any value under the support of the distribution describing the random variable, we can make sampling more difficult if an initial value is chosen in the extreme tail of the distribution, for example. If starting values are not passed by the user, default values are chosen from the mean, median or mode of the distribution.\n",
    "\n",
    "It is sometimes (but not always) useful to initialize a MCMC simulation at the maximum *a posteriori* (MAP) estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    \n",
    "    posterior_mode = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    trace = pm.sample(100, step=pm.Metropolis(), cores=2, initvals=posterior_mode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are sampling more than one Markov chain from our model, it is often recommended to initialize each chain to different starting values, so that lack of convergence can be more easily detected (see *Model Checking* section). \n",
    "\n",
    "### Storing samples\n",
    "\n",
    "Notice in the above call to `sample` that output is assigned to a variable we have called `trace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `InferenceData` object is a data structure that stores the samples from an MCMC run as grouped attributes. The data structure itself is an `xarray.Dataset` object, which is a dictionary-like object that stores the samples in a multi-dimensional array.\n",
    "\n",
    "The xarray components include:\n",
    "\n",
    "- **Data variables** are the actual values generated from the MCMC draws\n",
    "- **Dimensions** are the axes on which refer to the data variables\n",
    "- **Coordinates** are pointers to specific slices or points in the `xarray.Dataset`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel sampling\n",
    "\n",
    "Nearly all modern desktop computers have multiple CPU cores, and running multiple MCMC chains is an **embarrasingly parallel** computing task. It is therefore relatively simple to run chains in parallel in PyMC. This is done by setting the `cores` argument in `sample` to some value between 2 and the number of cores on your machine (you can specify more chains than cores, but you will not gain efficiency by doing so). The default value of `cores` is `None`, which will select the number of CPUs on your machine, to a maximum of 4. \n",
    "\n",
    "> Keep in mind that some chains might themselves be multithreaded via openmp or BLAS. In those cases it might be faster to set this to 1.\n",
    "\n",
    "By default, PyMC will run a sample a minimum of 2 and a maximum of `cores` chains. However, the number of chains sampled can be set independently of the number of cores by specifying the `chains` argument.\n",
    "\n",
    "```python\n",
    "with test_score_model:\n",
    "    trace = pm.sample(chains=4, cores=2)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running $n$ iterations with $c$ chains will result in $c \\times n$ samples.\n",
    "\n",
    "Generating several chains is generally recommended because it aids in model checking, allowing statistics such as the potential scale reduction factor ($\\hat{R}$) and effective sample size to be calculated.\n",
    "\n",
    "### Reproducible sampling\n",
    "\n",
    "A practical drawback of using stochastic sampling methods for statistical inference is that it can be more difficult to reproduce individual results, due to the fact that sampling involves the use of pseudo-random number generation. To aid in reproducibility (and debugging), it can be helpful to set a **random number seed** prior to sampling. The `random_seed` argument can be used to set PyMC's random number generator to a particular seed integer, which results in the same sequence of random numbers each time the seed is set to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    rtrace = pm.sample(100, cores=2, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrace.posterior['beta0'].sel(chain=0, draw=slice(0, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the same seed for another run of the same model will generate the same sequence of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with test_score_model:\n",
    "    rtrace = pm.sample(100, cores=2, random_seed=42)\n",
    "\n",
    "rtrace.posterior['beta0'].sel(chain=0, draw=slice(0, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Coal mining disasters\n",
    "\n",
    "Let's return to the coal mining disasters example from the previous section. Recall that we are interested in estimating the annyal rate of disasters, where there is thought to be a rate change at some point in the time series.\n",
    "\n",
    "Here again is the model:\n",
    "\n",
    "$$\n",
    "y_t \\sim \\operatorname{Poisson}(\\lambda_t), t=1851, \\ldots, 1962 \\\\\n",
    "\\lambda_t = \\left\\{ \\begin{array}{}\\lambda_1 \\text{ for } t \\leq \\tau \\\\ \\lambda_2 \\text{ for } t > \\tau \\end{array}\\right. \\\\\n",
    "\\lambda_j \\sim \\operatorname{Gamma}(1, 10) \\\\\n",
    "\\tau \\sim \\operatorname{DiscreteUniform}(1851, 1962)\n",
    "$$\n",
    "\n",
    "And an implementation in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters_array = np.array(\n",
    "    [4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3, \n",
    "     1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, \n",
    "     1, 3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, \n",
    "     0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2, 1, 1, 1, \n",
    "     1, 2, 4, 2, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, \n",
    "     1])\n",
    "years = np.arange(1851, 1962, dtype=int)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    early_lambda = pm.Gamma('early_lambda', 1, 10)\n",
    "    late_lambda = pm.Gamma('late_lambda', 1, 10)\n",
    "    change_point = pm.DiscreteUniform('change_point', 1851, 1962)\n",
    "    \n",
    "    lam = pm.Deterministic('lam', pm.math.where(years > change_point, late_lambda, early_lambda))\n",
    "    pm.Poisson('rate', lam, observed=disasters_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pm.sample` to draw 1000 samples, using the default step method assigned by PyMC. Then experiment with different step methods and observe the effect on sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX-based Samplers\n",
    "\n",
    "An alternative to PyMC's PyTensor-based samplers are samplers written in JAX. Using these samplers, all the operations needed to compute a posterior can be performed under JAX, reducing the Python overhead during sampling and leveraging all JAX performance improvements and features like the ability to sample on GPUs or TPUs.\n",
    "\n",
    "PyMC offers NUTS JAX samplers via [NumPyro](https://num.pyro.ai/en/latest/index.html) or [BlackJAX](https://blackjax-devs.github.io/blackjax/). Significantly, BlackJAX and NumPyro can both be used because in PyMC the modeling language is decoupled from the inference methods; BlackJAX and NumPyro only require a log-probability density function written in JAX. This demonstrates that samplers can be developed independently of PyMC and then be made available to users of the library.\n",
    "\n",
    "The JAX samplers can be invoked using the `nuts_sampler` argument for `pm.sample`:\n",
    "\n",
    "```python\n",
    "pm.sample(nuts_sampler=\"numpyro\")\n",
    "```\n",
    "\n",
    "### Exercise: Speed test\n",
    "\n",
    "Compare the speed of the JAX samplers to the PyTensor samplers using the coal mining disasters model above. \n",
    "\n",
    "HINT: You can use the `%%time` magic command to time the execution of a cell.\n",
    "\n",
    "ANOTHER HINT: You may need to reparameterize the model to use the JAX samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of Missing Data\n",
    "\n",
    "As with most textbook examples, the models we have examined so far assume that the associated data are complete. That is, there are no **missing values** corresponding to any observations in the dataset. However, many real-world datasets have missing observations, usually due to some logistical problem during the data collection process. The easiest way of dealing with observations that contain missing values is simply to exclude them from the analysis. However, this results in loss of information if an excluded observation contains valid values for other quantities, and can bias results. An alternative is to impute the missing values, based on information in the rest of the model.\n",
    "\n",
    "For example, consider a survey dataset for some wildlife species:\n",
    "\n",
    "    Count   Site   Observer   Temperature\n",
    "    ------- ------ ---------- -------------\n",
    "    15      1      1          15\n",
    "    10      1      2          NA\n",
    "    6       1      1          11\n",
    "\n",
    "Each row contains the number of individuals seen during the survey, along with three covariates: the site on which the survey was conducted, the observer that collected the data, and the temperature during the survey. If we are interested in modelling, say, population size as a function of the count and the associated covariates, it is difficult to accommodate the second observation because the temperature is missing (perhaps the thermometer was broken that day). Ignoring this observation will allow us to fit the model, but it wastes information that is contained in the other covariates.\n",
    "\n",
    "In a Bayesian modelling framework, missing data are accommodated simply by treating them as **unknown model parameters**. Values for the missing data $\\tilde{y}$ are estimated naturally, using the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "This describes additional data $\\tilde{y}$, which may either be considered unobserved data or potential future observations. We can use the posterior predictive distribution to model the likely values of missing data.\n",
    "\n",
    "Consider the coal mining disasters data introduced previously. Assume that two years of data are missing from the time series; we indicate this in the data array by the use of an arbitrary placeholder value, `-999`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters_missing = np.array([ 4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n",
    "3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n",
    "2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,\n",
    "1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n",
    "0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n",
    "3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n",
    "0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n",
    "\n",
    "N = len(disasters_missing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate these values in PyMC, we need to convert these placeholder values to `np.nan` (or `None`) values so that they can be handled by the model as missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters_missing = np.where(disasters_missing == -999, np.nan, disasters_missing)\n",
    "disasters_missing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array can then be passed to the model likelihood, which recognizes the `nan` values as missing and replaces them with stochastic variables of the desired type. For the coal mining disasters problem, recall that disaster events were modeled as Poisson variates:\n",
    "\n",
    "```python\n",
    "disasters = Poisson('disasters', mu=rate, observed=disasters_missing)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in `disasters` is a Poisson random variable, irrespective of whether the observation was missing or not. The difference is that actual observations are assumed to be data stochastics, while the missing\n",
    "values are unobserved stochastics. The latter are considered unknown, rather than fixed, and therefore estimated by the fitting algorithm, just as unknown model parameters are.\n",
    "\n",
    "The model is otherwise unchanged from the complete data case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as missing_data_model:\n",
    "\n",
    "    # Prior for distribution of switchpoint location\n",
    "    switchpoint = pm.Uniform('switchpoint', lower=0, upper=N)\n",
    "    # Priors for pre- and post-switch mean number of disasters\n",
    "    early_mean = pm.Exponential('early_mean', lam=1.)\n",
    "    late_mean = pm.Exponential('late_mean', lam=1.)\n",
    "\n",
    "    # Allocate appropriate Poisson rates to years before and after current\n",
    "    # switchpoint location\n",
    "    idx = np.arange(N)\n",
    "    rate = pm.math.switch(switchpoint >= idx, early_mean, late_mean)\n",
    "\n",
    "    # Data likelihood\n",
    "    disasters = pm.Poisson('disasters', rate, observed=disasters_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with missing_data_model:\n",
    "    trace_missing = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_model.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.summary(trace_missing, var_names=['disasters_missing'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Load the titanic dataset, and construct an appropriate model to predict passenger survival rate. Summarize the parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://raw.githubusercontent.com/fonnesbeck/bayes_course_2022/master/data/'\n",
    "\n",
    "try:\n",
    "    titanic = pd.read_excel('../data/titanic.xls')\n",
    "except FileNotFoundError:\n",
    "    titanic = pd.read_excel(DATA_URL + 'titanic.xls')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Ching & Chen. 2007. Transitional Markov chain Monte Carlo method for Bayesian model updating, model class selection and model averaging. Journal of Engineering Mechanics 2007\n",
    "2.\tHoffman MD, Gelman A. 2014. The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research. 15(1):1593-1623.\n",
    "3. M.I. Jordan. 2004. Graphical models. Statist. Sci., 19(1):140–155.\n",
    "4. Neal, R. M. 2003. Slice sampling. The Annals of Statistics, 31(3), 705–767. doi:10.1111/1467-9868.00198"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('bayes_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "vscode": {
   "interpreter": {
    "hash": "485c2aecfeff35fe97c500045cb91db26354005e32990317d3834cb0213a269e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
